{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55588557-bd2a-43f9-83fb-27b7f3404025",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"200\" src=\"./img/northwestern.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9667e-89ff-41a0-a963-1c75f5fd49c2",
   "metadata": {},
   "source": [
    "# 4. How to run the built-in HTRC algorithms on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30789f-7835-4090-afe6-c44811deecf9",
   "metadata": {},
   "source": [
    "There are four built-in algorithms in the HTRC:\n",
    "\n",
    "<ul>\n",
    "<li>Extracted Features Download Helper (v3.1)</li>\n",
    "<li>InPhO Topic Model Explorer (v1.0b225)</li>\n",
    "<li>Named Entity Recognizer (v2.0)</li>\n",
    "<li>Token Count and Tag Cloud Creator (v2.0)</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716be9a7-a5b6-4d35-8b0b-ad693b5a61a2",
   "metadata": {},
   "source": [
    "The <strong> Extracted Features Download Helper</strong> and the <strong>InPhO Topic Model Explorer</strong> are a more advanced tools so we are not going over them today. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643fdf6-2294-4bd2-9922-e6a4e548cc75",
   "metadata": {},
   "source": [
    "## Token Count and Tag Cloud Creator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50447e4-995e-43c0-8012-5b81f8196e50",
   "metadata": {},
   "source": [
    "The first tool we will use is the <strong>Token Count and Tag Cloud Creator</strong>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daeaee-50cc-4740-a28d-664aecbb1af0",
   "metadata": {},
   "source": [
    "What HTRC says: *Identify the tokens (words) that occur most often in a workset and the number of times they occur. Create a tag cloud visualization of the most frequently occurring words in a workset, where the size of the word is displayed in proportion to the number of times it occurred. Can be run on worksets of fewer than 3000 volumes, as long as the total size of the workset is less than 3 GB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac096441-49dd-402d-9a6f-3b0a1a1ba4dd",
   "metadata": {},
   "source": [
    "Token: Part of text analysis is tokenizing--turning words into tokens (words, numbers, punctuation) that a computer can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf513b-02c0-41a0-8a7c-51e6a729671b",
   "metadata": {},
   "source": [
    "Limits: \n",
    "    Only works on small worksets (fewer than 3000 volumes and less than 3GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d72bf6-9f64-4e53-99bf-66e458f9c9ec",
   "metadata": {},
   "source": [
    "Before running the algorithm you need to fill out the form. We will go through the parts marked 1-5 below the screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16416220-7f24-4568-896d-56929188f3f0",
   "metadata": {},
   "source": [
    "![tag cloud form](./img/tag-cloud-form.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f343c8d-9465-4250-b3c3-e53b6a9cb701",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Stop Words\n",
    "[Stop words](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) are a list of words you want the algorithm to ignore. Typically, these will be filler words that do no impact the tone of the work, so it is best to remove the term. Words like \"the,\" \"name,\" \"a,\" \"ask\" are common stop words. HathiTrust does allow for customization, but for now we will use the default list. Customization is most important when working with non-English worksets. \n",
    "\n",
    "More stop word lists:\n",
    "- https://countwordsfree.com/stopwords\n",
    "- https://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee06a42-64b4-477f-906b-af2f53d3fe41",
   "metadata": {},
   "source": [
    "### 2. Replacement Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f199-ad5f-4e8d-b080-b370f50479fa",
   "metadata": {},
   "source": [
    "There are common mistakes made by OCR tools. Mistakes like mistaking \"0\" for an \"O.\" This is a list of common replacements. Like stop words, HTRC allows for a custom file. In this workshop we will use the default list. Customization is necessary for non-English worksets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e16e5-e069-4d55-99a3-d910cdfab4dd",
   "metadata": {},
   "source": [
    "### 3. Lowercase tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc900879-b5fe-4a8e-9819-e204f9fad4c1",
   "metadata": {},
   "source": [
    "This will almost always be a \"yes.\" Otherwise, words will be counted separately just based on capitalization. Lowercasing means \"football\" and \"Football\" is treated the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32c271-fff0-458d-a96c-78919b6811bc",
   "metadata": {},
   "source": [
    "### 4. Display only tokens that match this regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1b579-00a1-4746-afc2-e744d3606f0d",
   "metadata": {},
   "source": [
    "Regular expression patterns (regex) is a powerful text mining tool. It is highly recommended you learn how to use them. In general, this can be left as default. \n",
    "\n",
    "More on Regex:\n",
    "- https://regexone.com/ is an online quiz that teaches how to do regex\n",
    "- https://regex101.com/ for writing regex recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f13a88-cabf-4263-874d-0f3657a21ecb",
   "metadata": {},
   "source": [
    "To make things a little more interesting I tried this regex pattern: `^.*ing$`. It will capture any word that ends with \"ing\" to capture verbs (imperfectly of course, since English contains verbs that end in other letters). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52c7ee-1512-41fd-9406-33b1998024b2",
   "metadata": {},
   "source": [
    "### 5. Max number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdca26-cf1d-4fab-9e9b-0ac861907293",
   "metadata": {},
   "source": [
    "It is unclear what the max number of tokens can be, but 200 is a good amount. Otherwise the word cloud stops functioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42e20c-8c82-4860-a8bd-2422e8814885",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44e31f-6b2b-4f36-92f0-acc3162d57d8",
   "metadata": {},
   "source": [
    "Depending on how big the workset, it will take some time to see a result. Active and completed job are under Algorithms >  Jobs > Complete Job \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cfee8-2137-44d7-bf21-059e91f52bc8",
   "metadata": {},
   "source": [
    "While the wordcloud has token limits, HTRC allows for a download of the full token count list (token_count.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed7479-8c2f-4382-a0c2-ffaf24a4eed4",
   "metadata": {},
   "source": [
    "![tag cloud form](./img/tagCloud.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa14aa-b1dc-4c67-a0ea-0db93b4cc099",
   "metadata": {},
   "source": [
    "The word cloud should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae475842-c99d-4d8b-b168-f07e13927d79",
   "metadata": {},
   "source": [
    "![tag cloud form](./img/wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7e81e-8dad-4b35-8273-f08d5d6a8a34",
   "metadata": {},
   "source": [
    "## Named Entity Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f3a3d-736f-498c-8707-6744fb1ac81c",
   "metadata": {},
   "source": [
    "The **Named Entity Recognizer** is a straightforward algorithm with the capability to not just extract entities in the workset, but also identify their type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81e241-dcd6-45c3-8bc0-47839a6a5dbc",
   "metadata": {},
   "source": [
    "**Entity:** Applies logic to tokens that identifies words that go together. For example, entity recognition can identify the tokens \"Lake\" and \"Michigan\" is one entity, \"Lake Michigan.\" This tool can also identify \"Lake Michigan\" as entity type \"Location.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04698c81-3f07-4df8-a960-e7d3bdb9be38",
   "metadata": {},
   "source": [
    "What HTRC says: *Generate a list of all of the names of people and places, as well as dates, times, percentages, and monetary terms, found in a workset. You can choose which entities you would like to extract. Can be run on worksets of fewer than 3000 volumes, as long as the total size of the workset is less than 3 GB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965456e-073c-425c-bc9d-13f9b6731584",
   "metadata": {},
   "source": [
    "As we can see from the sample result below, it does a pretty great job extracting entities and accurately labeling type!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d9f39-d436-4947-9a0a-d466875353b6",
   "metadata": {},
   "source": [
    "![named entity table](./img/named-entity-table.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cdf54f-f0cd-4dfa-b606-d0553ab39b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
